{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContextualBandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        \n",
    "        # Each bandit has 4 arms. We use 3 bandits.\n",
    "        # The highest value in each column is the best arm for that bandit\n",
    "        self.bandits = np.array([\n",
    "                [0.2, 0.0, 0.0, -5.0],\n",
    "                [0.1, -5.0, 1.0, 0.25],\n",
    "                [-5.0, 5.0, 5.0, 5.0]\n",
    "            ])\n",
    "        \n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        # Return a random bandit. The bandit represents the state.\n",
    "        # Basically we shut our eyes and wander over to a random\n",
    "        # bandit which we will play.\n",
    "        self.state = np.random.randint(0, self.num_bandits)\n",
    "        return self.state\n",
    "\n",
    "    def pullArm(self, action):\n",
    "        chance = self.bandits[self.state, action]\n",
    "        reward = 1 if np.random.randn(1) > chance else -1\n",
    "        return reward\n",
    "        \n",
    "        \n",
    "class agent():\n",
    "    def __init__(self, learn_rate, state_size, action_size):\n",
    "        # Create the feed-forward part of the neural network.\n",
    "        # The agent is given a state as input and will produce\n",
    "        # an action.\n",
    "        \n",
    "        # The network takes as input the state, which is an integer\n",
    "        # The slim.one_hot_encoding function maps the input state\n",
    "        # to a 1 in at the integer position in a one-hot vector\n",
    "        self.state_in = tf.placeholder(dtype=tf.int32,\n",
    "                                       shape=[1])\n",
    "        state_in_one_hot = slim.one_hot_encoding(labels=self.state_in,\n",
    "                                                 num_classes=state_size)\n",
    "        \n",
    "        output = slim.fully_connected(inputs=state_in_one_hot,\n",
    "                                      num_outputs=action_size,\n",
    "                                      biases_initializer=None,\n",
    "                                      activation_fn=tf.nn.sigmoid,\n",
    "                                      weights_initializer=tf.ones_initializer())\n",
    "        \n",
    "        self.output = tf.reshape(tensor=output,\n",
    "                                 shape=[-1])\n",
    "        \n",
    "        self.chosen_action = tf.argmax(input=self.output,\n",
    "                                       axis=0)\n",
    "        \n",
    "        # The next six lines establish the training procedure.\n",
    "        # We feed the reward and chosen action into the network\n",
    "        # to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(dtype=tf.float32,\n",
    "                                            shape=[1])\n",
    "        self.action_holder = tf.placeholder(dtype=tf.int32,\n",
    "                                            shape=[1])\n",
    "        self.responsible_weight = tf.slice(input_=self.output,\n",
    "                                           begin=self.action_holder,\n",
    "                                           size=[1])\n",
    "        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learn_rate)\n",
    "        self.update = optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   -0.25  0.  ]\n",
      "[43.5  35.25 34.  ]\n",
      "[80.5  75.25 69.5 ]\n",
      "[116.   112.5  107.25]\n",
      "[154.5  149.25 145.  ]\n",
      "[190.75 188.75 183.25]\n",
      "[223.   228.25 215.  ]\n",
      "[263.75 268.5  248.  ]\n",
      "[297.25 309.25 285.25]\n",
      "[337.25 344.75 321.25]\n",
      "[377.25 378.5  361.5 ]\n",
      "[417.5  416.75 401.  ]\n",
      "[460.   451.25 435.5 ]\n",
      "[499.75 487.   470.  ]\n",
      "[538.5  522.25 509.5 ]\n",
      "[576.75 560.25 546.25]\n",
      "[614.   600.   583.25]\n",
      "[650.75 640.25 617.75]\n",
      "[692.25 676.   656.  ]\n",
      "[731.25 714.75 692.75]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create a contextual bandit\n",
    "context_bandit = ContextualBandit()\n",
    "# Load the agent\n",
    "my_agent = agent(learn_rate=0.001,\n",
    "                 state_size=context_bandit.num_bandits,\n",
    "                 action_size=context_bandit.num_actions)\n",
    "\n",
    "# Weights that we can use to look into the network\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "# Total number of episodes to perform training\n",
    "total_episodes = 10000\n",
    "\n",
    "# Total rewards for each (bandit, arm)\n",
    "total_reward = np.zeros([context_bandit.num_bandits,\n",
    "                         context_bandit.num_actions])\n",
    "\n",
    "# Chance of taking a random action\n",
    "e = 0.1\n",
    "\n",
    "# Get a TF Op that will initialise all the variables when run\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    # Run initialiser Op\n",
    "    sess.run(init)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        # Get a state from the environment. Here this means the\n",
    "        # enumeration of a bandit\n",
    "        state = context_bandit.getBandit()\n",
    "        \n",
    "        # Choose either a random action or use our network to produce\n",
    "        # one\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(context_bandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(my_agent.chosen_action,\n",
    "                              feed_dict={my_agent.state_in:[state]})\n",
    "            \n",
    "        # Get the reward for the chosen action\n",
    "        reward = context_bandit.pullArm(action)\n",
    "        \n",
    "        # Update the agent network\n",
    "        feed_dict = {my_agent.reward_holder:[reward],\n",
    "                     my_agent.action_holder:[action],\n",
    "                     my_agent.state_in:[state]}\n",
    "        _, ww = sess.run([my_agent.update, weights],\n",
    "                         feed_dict=feed_dict)\n",
    "        \n",
    "        # Update our running score\n",
    "        total_reward[state, action] += reward\n",
    "        if ep % 500 == 0:\n",
    "            print(np.mean(total_reward, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a contextual bandit\n",
    "context_bandit = ContextualBandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Variable.value of <tf.Variable 'fully_connected/weights:0' shape=(3, 4) dtype=float32_ref>>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
