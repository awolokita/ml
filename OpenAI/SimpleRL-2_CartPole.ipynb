{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation as anm\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  env_render(env_vis):\n",
    "    plt.figure()\n",
    "    plot  =  plt.imshow(env_vis[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def  animate(i):\n",
    "        plot.set_data(env_vis[i])\n",
    "        anim  =  anm.FuncAnimation(plt.gcf(), animate, frames=len(env_vis), interval=20, repeat=True, repeat_delay=20)\n",
    "        display(display_animation(anim,  default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(r.size)):\n",
    "        running_sum = running_sum * gamma + r[i]\n",
    "        discounted_r[i] = running_sum\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, learning_rate, state_size,\n",
    "                 action_size, hidden_layer_size):\n",
    "        \n",
    "        # These four lines establish the feed-forward part of the network.\n",
    "        # The agent takes a state and produces an action.\n",
    "        self.state_input = tf.placeholder(shape=[None, state_size],\n",
    "                                          dtype=tf.float32)\n",
    "        \n",
    "        hidden_layer = slim.fully_connected(inputs=self.state_input,\n",
    "                                            num_outputs=hidden_layer_size,\n",
    "                                            activation_fn=tf.nn.relu,\n",
    "                                            biases_initializer=None)\n",
    "        \n",
    "        self.output = slim.fully_connected(inputs=hidden_layer,\n",
    "                                           num_outputs=action_size,\n",
    "                                           activation_fn=tf.nn.softmax,\n",
    "                                           biases_initializer=None)\n",
    "        \n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        # These next six lines establish the training proceedure.\n",
    "        # We feed the reward and chosen action into the network\n",
    "        # in order to compute the loss and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None],\n",
    "                                            dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],\n",
    "                                            dtype=tf.int32)\n",
    "        # Not 100% sure yet what these ops do.\n",
    "        # Something to with reordering the output tensor which may be 2D\n",
    "        # if multiple states are pushed through\n",
    "        o_shape = tf.shape(self.output)\n",
    "        self.indexes = tf.range(0, o_shape[0]) * o_shape[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]),\n",
    "                                             self.indexes)\n",
    "        \n",
    "        # Policy loss function\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        # Make a list of placeholder: each element is a placeholder\n",
    "        # for a trainable variable in the network\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for i, x in enumerate(trainable_variables):\n",
    "            placeholder = tf.placeholder(dtype=tf.float32,\n",
    "                                         name=str(i)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        # Create a bunch of gradients, one for each trainable variable\n",
    "        self.gradients = tf.gradients(self.loss, trainable_variables)\n",
    "        \n",
    "        # Define an optimizer.\n",
    "        # We define the model update function using the apply_gradients\n",
    "        # function instead of using minimize as that allows us to\n",
    "        # control the number of steps taken per update.\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,\n",
    "                                                          trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 23.000000\n",
      "Episode 100: 24.840000\n",
      "Episode 200: 32.450000\n",
      "Episode 300: 36.510000\n",
      "Episode 400: 61.610000\n",
      "Episode 500: 107.540000\n",
      "Episode 600: 136.880000\n",
      "Episode 700: 162.940000\n",
      "Episode 800: 179.490000\n",
      "Episode 900: 180.460000\n",
      "Episode 1000: 188.860000\n",
      "Episode 1100: 189.090000\n",
      "Episode 1200: 180.800000\n",
      "Episode 1300: 174.900000\n",
      "Episode 1400: 175.650000\n",
      "Episode 1500: 165.100000\n",
      "Episode 1600: 175.870000\n",
      "Episode 1700: 172.590000\n",
      "Episode 1800: 156.940000\n",
      "Episode 1900: 174.510000\n",
      "Episode 2000: 192.380000\n",
      "Episode 2100: 198.030000\n",
      "Episode 2200: 198.450000\n",
      "Episode 2300: 199.990000\n",
      "Episode 2400: 198.770000\n",
      "Episode 2500: 195.670000\n",
      "Episode 2600: 198.600000\n",
      "Episode 2700: 199.230000\n",
      "Episode 2800: 199.660000\n",
      "Episode 2900: 199.560000\n",
      "Episode 3000: 199.480000\n",
      "Episode 3100: 199.320000\n",
      "Episode 3200: 198.050000\n",
      "Episode 3300: 199.020000\n",
      "Episode 3400: 197.700000\n",
      "Episode 3500: 194.990000\n",
      "Episode 3600: 199.130000\n",
      "Episode 3700: 199.060000\n",
      "Episode 3800: 195.220000\n",
      "Episode 3900: 195.180000\n",
      "Episode 4000: 199.450000\n",
      "Episode 4100: 194.690000\n",
      "Episode 4200: 196.970000\n",
      "Episode 4300: 198.710000\n",
      "Episode 4400: 199.960000\n",
      "Episode 4500: 200.000000\n",
      "Episode 4600: 199.720000\n",
      "Episode 4700: 200.000000\n",
      "Episode 4800: 198.920000\n",
      "Episode 4900: 199.590000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "myAgent = agent(learning_rate=1e-2,\n",
    "                state_size=n_states,\n",
    "                action_size=n_actions,\n",
    "                hidden_layer_size=8)\n",
    "\n",
    "total_episodes = 5000\n",
    "max_steps_per_episode = 999\n",
    "network_update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "env_vis = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    gradient_buffer = [x*0 for x in sess.run(tf.trainable_variables())]\n",
    "    \n",
    "    for episode in range(total_episodes):\n",
    "        state = env.reset()\n",
    "        running_reward = 0\n",
    "        episode_history = []\n",
    "        for j in range(max_steps_per_episode):\n",
    "            if episode % 100 == 0 and episode != 0:\n",
    "                env.render()\n",
    "            # Probabilistically choose an action give the network outputs.\n",
    "            action_distribution = sess.run(myAgent.output,\n",
    "                                           feed_dict={myAgent.state_input:[state]})\n",
    "            action = np.random.choice(action_distribution[0],\n",
    "                                      p=action_distribution[0])\n",
    "            action = np.argmax(action_distribution == action)\n",
    "            \n",
    "            # Perform the action in the environment and get new state\n",
    "            # and reward\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            episode_history.append([state, action, reward, new_state])\n",
    "            state = new_state\n",
    "            running_reward += reward\n",
    "            if j == max_steps_per_episode-1:\n",
    "                print('foo')\n",
    "            \n",
    "            if done == True:\n",
    "                # Episode finished\n",
    "                # Process the rewards into discounted form and then run the\n",
    "                # feed forward network again to get the gradients produced\n",
    "                # by the experiences and the reward. These gradients are\n",
    "                # added to the gradient buffers, which will be used to update\n",
    "                # the network in one batch.\n",
    "                episode_history = np.array(episode_history)\n",
    "                episode_history[:,2] = discount_rewards(episode_history[:,2])\n",
    "                feed_dict = {myAgent.reward_holder:episode_history[:,2],\n",
    "                             myAgent.action_holder:episode_history[:,1],\n",
    "                             myAgent.state_input:np.vstack(episode_history[:,0])}\n",
    "                gradients = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "                gradient_buffer = [gradient_buffer[i] + x for i, x in enumerate(gradients)]\n",
    "                \n",
    "                # If we've run the prescribed number of episodes then\n",
    "                # we can update the network weights with the buffered\n",
    "                # gradients. Once we've updated the network we reset the\n",
    "                # gradient buffer to 0.\n",
    "                if episode % network_update_frequency == 0 and episode != 0:\n",
    "                    feed_dict = dict(zip(myAgent.gradient_holders,\n",
    "                                         gradient_buffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    gradient_buffer = [x*0 for x in gradient_buffer]\n",
    "                \n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(j)\n",
    "                break\n",
    "                \n",
    "            # Update our running tally of scores\n",
    "        if episode % 100 == 0:\n",
    "            print(\"Episode %i: %f\" % (episode, np.mean(total_reward[-100:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
