{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn import svm\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "DATA_FILE = './adult.data'\n",
    "TEST_FILE = './adult.test'\n",
    "NAMES = ['age', 'workclass','fnlwgt','education','education-num','marital-status',\n",
    "         'occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_prediction(theta, X):\n",
    "    h = np.dot(X, theta)\n",
    "    return sigmoid(h)\n",
    "\n",
    "\n",
    "def logistic_regression_cost_grad(features, target, theta, regu=0):\n",
    "    #TODO: add regularisation\n",
    "    # Theta is a column vector of dim (n)\n",
    "    # features is a dim (m x n) matrix\n",
    "    # target is a column vector of dim (m)\n",
    "    m = len(features)\n",
    "    h = logistic_prediction(theta, features)\n",
    "    # Cost\n",
    "    J = -(1/m) * (np.dot(np.log(h).T, target) + np.dot(np.log(1-h).T, (1-target)))\n",
    "    # Gradient\n",
    "    grad = np.dot(features.T,(h-target))/m\n",
    "    return float(J), grad\n",
    "\n",
    "\n",
    "def preprocess_data(data_df, feature_list=None, mean=None, std=None):\n",
    "    d = {}\n",
    "    \n",
    "    # Number of samples\n",
    "    d['num_samples'] = len(data_df)\n",
    "    \n",
    "    # Replace '?' with NaN\n",
    "    data_df = data_df.replace(' ?', np.nan)\n",
    "    \n",
    "    # Find how many rows have missing values, indicated by a '?'\n",
    "    element_idx_missing_data = data_df.isnull()\n",
    "    row_idx_missing_data = element_idx_missing_data.any(axis=1)\n",
    "    num_samples_missing_data = sum(row_idx_missing_data)\n",
    "    d['num_samples_missing_data'] = num_samples_missing_data\n",
    "    \n",
    "    # Remove samples with missing data\n",
    "    data_df = data_df.dropna()\n",
    "    \n",
    "    # We can remove the eduction column as this is already nicely encoded\n",
    "    # numerically in the education-num column.\n",
    "    data_df = data_df.drop('education', axis=1)\n",
    "    \n",
    "    # Now we need to deal with the other categorical features which aren't\n",
    "    # easily encoded as numerical values. We choose to use one-hot encoding,\n",
    "    # because it will avoid any \"confusion\" that the algorithm may have\n",
    "    # with numerical encodings of categories. The one-hot encoding will\n",
    "    # effectively \"unroll\" all of the categorical features into unique features\n",
    "    # corresponding to their possible values. For example the 'sex' feature\n",
    "    # can take values 'Male' and 'Female'. This feature will become a 2D vector\n",
    "    # ['sex_Male', 'sex_Female']; a feature [0, 1] denotes a 'Female' sample.\n",
    "    # We want to get one-hot vectors for: sex, workclass, marital-status, occupation,\n",
    "    # relationship, race, native-country, income.\n",
    "    # Use a funky separator ('$_$') to be able to pick out the one-hot values later.\n",
    "    dummy_cols = ['sex','workclass', 'marital-status', 'occupation', 'relationship', 'race', 'native-country', 'income']\n",
    "    data_df = pd.get_dummies(data_df,columns=dummy_cols, prefix_sep='$_$')\n",
    "    \n",
    "    # Extract the target vector: income. We previously transformed this into a one-hot,\n",
    "    # however we want this to be a binary scalar. The one-hot transformation will\n",
    "    # help us to easily make a binary value using boolean operators.\n",
    "    # Income <= 50K -> 0, income > 50K -> 1.\n",
    "    try:\n",
    "        data_df.rename(columns = {'income$_$ >50K.':'income$_$ >50K'}, inplace = True)\n",
    "        data_df.rename(columns = {'income$_$ <=50K.':'income$_$ <=50K'}, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "    target = data_df['income$_$ >50K']\n",
    "    \n",
    "    # Now drop the income vectors from the data frame to leave us with just the \n",
    "    # features.\n",
    "    data_df = data_df.drop(['income$_$ <=50K', 'income$_$ >50K'], axis=1)\n",
    "    \n",
    "    d['feature_list'] = list(data_df)\n",
    "    \n",
    "    if feature_list is not None:\n",
    "        # If there features are missing, add them\n",
    "        for x in feature_list:\n",
    "            if x not in list(data_df):\n",
    "                data_df[x] = pd.Series([0 for _ in range(len(data_df))])\n",
    "        for x in list(data_df):\n",
    "            if x not in feature_list:\n",
    "                data_df.drop(x)\n",
    "        # Rearrange to ensure that the features are in the same order\n",
    "        data_df = data_df[feature_list]\n",
    "\n",
    "    # Since the data is a collection of one-hots and continuous values of different\n",
    "    # ranges, let's do some normalisation and scaling.\n",
    "    # We'll try two different normalisation schemes: normalise everything, and\n",
    "    # only normalise continuous (non-one-hot) values.=\n",
    "    data_mean = data_df.mean() if (mean is None) else mean\n",
    "    data_std = data_df.std() if (std is None) else std\n",
    "    data_df_norm_all = data_df - data_mean\n",
    "    data_df_norm_all = data_df_norm_all / data_std\n",
    "    d['mean'] = data_mean\n",
    "    d['std'] = data_std\n",
    "    # Do the continuous columns in a dumb way\n",
    "    #non_oh_cols = [x for x in list(data_df) if '$_$' not in x]\n",
    "    #data_df_norm_cont = data_df.copy()\n",
    "    #for col in  non_oh_cols:\n",
    "    #    data_df_norm_cont[col] = data_df[col] - data_df[col].mean()\n",
    "    #    data_df_norm_cont[col] = data_df_norm_cont[col] / data_df[col].std()\n",
    "        \n",
    "    # We now have two normalised feature sets. Let's do some training.\n",
    "    # First extend the features to have a column of 1s (bias) in the first column\n",
    "    #data_df_norm_all.insert(0, 'bias', [1 for _ in range(len(data_df_norm_all))])\n",
    "    #data_df_norm_cont.insert(0, 'bias', [1 for _ in range(len(data_df_norm_cont))])\n",
    "    \n",
    "    # Make features matrix\n",
    "    features = data_df_norm_all.as_matrix()\n",
    "    target = np.array(target, ndmin=2).T\n",
    "    d['features'] = features\n",
    "    d['target'] = target\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the data in from the \"adult\" dataset\n",
    "train_df = pd.read_csv(DATA_DIR + DATA_FILE, names=NAMES)\n",
    "test_df = pd.read_csv(DATA_DIR + TEST_FILE, names=NAMES, skiprows=1)\n",
    "\n",
    "training_data_dict = preprocess_data(train_df)\n",
    "test_data_dict = preprocess_data(test_df,\n",
    "                                 feature_list=training_data_dict['feature_list'],\n",
    "                                 mean=training_data_dict['mean'],\n",
    "                                 std=training_data_dict['std'])\n",
    "\n",
    "# Create some parameter vectors for each feature set. Initialise to 0\n",
    "num_features = training_data_dict['features'].shape[1]\n",
    "theta = np.zeros((num_features, 1))\n",
    "\n",
    "# Train\n",
    "train_features = training_data_dict['features']\n",
    "train_target = training_data_dict['target']\n",
    "\n",
    "test_features = test_data_dict['features']\n",
    "test_target = test_data_dict['target']\n",
    "\n",
    "alpha = 0.03\n",
    "num_iterations = 10000\n",
    "Jtrain_array = []\n",
    "Jtest_array = []\n",
    "for iteration in range(num_iterations):\n",
    "    J, grad = logistic_regression_cost_grad(test_features, test_target, theta)\n",
    "    Jtest_array.append(J)\n",
    "    J, grad = logistic_regression_cost_grad(train_features, train_target, theta)\n",
    "    Jtrain_array.append(J)\n",
    "    \n",
    "    # Update parameters\n",
    "    theta = theta - alpha * grad\n",
    "    \n",
    "    if (iteration % 500 == 0):\n",
    "        print(iteration, 'of', num_iterations, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "train_features = training_data_dict['features']\n",
    "train_target = training_data_dict['target']\n",
    "prediction = logistic_prediction(theta, train_features)\n",
    "correct_training_predictions = np.logical_not(np.logical_xor(prediction > 0.5, train_target > 0.5))\n",
    "train_result_df = pd.DataFrame()\n",
    "train_result_df['predictions'] = pd.Series(prediction.flatten()>0.5)\n",
    "train_result_df['target'] = pd.Series(train_target.flatten() > 0.5)\n",
    "train_result_df['result'] = correct_training_predictions\n",
    "print(\"Success rate on training data:\", train_result_df.result.sum()/len(train_result_df))\n",
    "\n",
    "# Predict on test set\n",
    "test_features = test_data_dict['features']\n",
    "test_target = test_data_dict['target']\n",
    "prediction = logistic_prediction(theta, test_features)\n",
    "correct_test_predictions = np.logical_not(np.logical_xor(prediction > 0.5, test_target > 0.5))\n",
    "test_result_df = pd.DataFrame()\n",
    "test_result_df['predictions'] = pd.Series(prediction.flatten()>0.5)\n",
    "test_result_df['target'] = pd.Series(test_target.flatten() > 0.5)\n",
    "test_result_df['result'] = correct_test_predictions\n",
    "print(\"Success rate on test data:\", test_result_df.result.sum()/len(test_result_df))\n",
    "\n",
    "# Show predictors\n",
    "s = pd.Series(theta[1:].flatten(), index=training_data_dict['feature_list'])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 3): display(s.sort_values(ascending=False))\n",
    "\n",
    "# Plot\n",
    "plt.plot(Jtrain_array)\n",
    "plt.plot(Jtest_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = sklearn.svm.SVC()\n",
    "clf.fit(train_features, train_target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = training_data_dict['features']\n",
    "train_target = training_data_dict['target']\n",
    "prediction = []\n",
    "idx = []\n",
    "for i, feature in enumerate(train_features):\n",
    "    if np.isnan(feature).any():\n",
    "        continue\n",
    "    idx.append(i)\n",
    "    prediction.append(clf.predict(feature.reshape(1, -1)))\n",
    "correct_test_predictions = np.logical_not(np.logical_xor(np.array(prediction) > 0.5, train_target[idx] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Success rate on test data:\", correct_test_predictions.sum()/len(correct_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "#train_features = training_data_dict['features']\n",
    "#train_target = training_data_dict['target']\n",
    "#prediction = clf.predict(train_features)\n",
    "#correct_training_predictions = np.logical_not(np.logical_xor(prediction > 0.5, train_target > 0.5))\n",
    "#train_result_df = pd.DataFrame()\n",
    "#train_result_df['predictions'] = pd.Series(prediction.flatten()>0.5)\n",
    "#train_result_df['target'] = pd.Series(train_target.flatten() > 0.5)\n",
    "#train_result_df['result'] = correct_training_predictions\n",
    "#print(\"Success rate on training data:\", train_result_df.result.sum()/len(train_result_df))\n",
    "\n",
    "# Predict on test set\n",
    "test_features = test_data_dict['features']\n",
    "test_target = test_data_dict['target']\n",
    "prediction = []\n",
    "idx = []\n",
    "for i, feature in enumerate(test_features):\n",
    "    if np.isnan(feature).any():\n",
    "        continue\n",
    "    idx.append(i)\n",
    "    prediction.append(clf.predict(feature.reshape(1, -1)))\n",
    "correct_test_predictions = np.logical_not(np.logical_xor(np.array(prediction) > 0.5, test_target[idx] > 0.5))\n",
    "#test_result_df = pd.DataFrame()\n",
    "#test_result_df['predictions'] = pd.Series(prediction.flatten()>0.5)\n",
    "#test_result_df['target'] = pd.Series(test_target.flatten() > 0.5)\n",
    "#test_result_df['result'] = correct_test_predictions\n",
    "#print(\"Success rate on test data:\", test_result_df.result.sum()/len(test_result_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Success rate on test data:\", correct_test_predictions.sum()/len(correct_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | Accuracy: 0.735795 | Cost: 0.693138\n",
      "Iteration 250 | Accuracy: 0.835509 | Cost: 0.379806\n",
      "Iteration 500 | Accuracy: 0.840731 | Cost: 0.351564\n",
      "Iteration 750 | Accuracy: 0.843549 | Cost: 0.341942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-be88832beac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m                             \u001b[0mreg_lambda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0mtrain_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                             num_epochs)\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Labmda: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initial cost: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-be88832beac9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, x, y, learning_rate, reg_lambda, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mcost_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_remainder\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-be88832beac9>\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, sess, _x, _y, learning_rate, reg_lambda)\u001b[0m\n\u001b[1;32m     53\u001b[0m                      self.reg_lambda: reg_lambda}\n\u001b[1;32m     54\u001b[0m         cost, _ = sess.run([self.cost, self.update],\n\u001b[0;32m---> 55\u001b[0;31m                            feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Implement logistic regression with TF\n",
    "class TFLogisticRegression():\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        \n",
    "        # Classifier graph\n",
    "        self.input_node = tf.placeholder(dtype=tf.float32,\n",
    "                          shape=[num_features, None],\n",
    "                          name=\"feature_ph\")\n",
    "        self.weights = tf.Variable(dtype=tf.float32,\n",
    "                           initial_value=tf.zeros([num_features, 1]),\n",
    "                           trainable=True,\n",
    "                           name=\"weights\")\n",
    "        self.bias = tf.Variable(dtype=tf.float32,\n",
    "                                initial_value=tf.zeros([1]),\n",
    "                                trainable=True,\n",
    "                                name=\"bias\")\n",
    "        \n",
    "        self.logits = tf.matmul(self.input_node,\n",
    "                                self.weights,\n",
    "                                transpose_a=True, \n",
    "                                name=\"logits\") + self.bias\n",
    "        self.prediction = tf.sigmoid(self.logits,\n",
    "                                     name=\"prediction_op\")\n",
    "        \n",
    "        # Learning graph\n",
    "        self.target = tf.placeholder(dtype=tf.float32,\n",
    "                                     shape=[None,num_classes],\n",
    "                                     name=\"target_ph\")\n",
    "        self.cost = tf.losses.sigmoid_cross_entropy(self.target,\n",
    "                                                    logits=self.logits)\n",
    "        self.reg_lambda = tf.placeholder(dtype=tf.float32,\n",
    "                                         name=\"lambda\")\n",
    "        self.cost = self.cost + self.reg_lambda * tf.nn.l2_loss(self.weights)\n",
    "        self.learning_rate = tf.placeholder(dtype=tf.float32,\n",
    "                                         name=\"learning_rate\")\n",
    "        self.update = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.update = self.update.minimize(self.cost)\n",
    "\n",
    "    def initialise_graph(self, sess):\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "    def predict(self, sess, x):\n",
    "        return sess.run(self.prediction,\n",
    "                        feed_dict={self.input_node:x})\n",
    "    \n",
    "    \n",
    "    def _train_step(self, sess, _x, _y, learning_rate, reg_lambda):\n",
    "        feed_dict = {self.input_node: _x,\n",
    "                     self.target: _y,\n",
    "                     self.learning_rate: learning_rate,\n",
    "                     self.reg_lambda: reg_lambda}\n",
    "        cost, _ = sess.run([self.cost, self.update],\n",
    "                           feed_dict=feed_dict)\n",
    "        return cost\n",
    "    \n",
    "    def train(self, sess, x, y, learning_rate, reg_lambda, batch_size, num_epochs):\n",
    "        num_batches = int(np.floor(x.shape[0]/batch_size))\n",
    "        batch_remainder = x.shape[0] % batch_size\n",
    "        cost_list = []\n",
    "        for i in tqdm_notebook(range(num_epochs), desc='Training'):\n",
    "            for j in range(num_batches):\n",
    "                k = j * batch_size\n",
    "                _x = np.matrix(x[k:k+batch_size-1]).T\n",
    "                _y = np.matrix(y[k:k+batch_size-1])\n",
    "                cost = self._train_step(sess, _x, _y, learning_rate, reg_lambda)\n",
    "                cost_list.append(cost)\n",
    "            if (batch_remainder > 0):\n",
    "                # Do remainder\n",
    "                k = -batch_remainder\n",
    "                _x = np.matrix(x[k:]).T\n",
    "                _y = np.matrix(y[k:])\n",
    "                cost = self._train_step(sess, _x, _y, learning_rate, reg_lambda)\n",
    "                cost_list.append(cost)\n",
    "                \n",
    "            if i % 250 == 0:\n",
    "                p = self.predict(sess, x.T)\n",
    "                pr = np.logical_not(np.logical_xor(p > 0.5, y > 0.5))\n",
    "                #print(pr[:10])\n",
    "                accuracy = pr.sum()/len(pr)\n",
    "                tqdm.write(\"Iteration %i | Accuracy: %f | Cost: %f\" % (i, accuracy, cost_list[-1]))\n",
    "\n",
    "        return cost_list\n",
    "        \n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR + DATA_FILE, names=NAMES)\n",
    "test_df = pd.read_csv(DATA_DIR + TEST_FILE, names=NAMES, skiprows=1)\n",
    "\n",
    "training_data_dict = preprocess_data(train_df)\n",
    "test_data_dict = preprocess_data(test_df,\n",
    "                                 feature_list=training_data_dict['feature_list'],\n",
    "                                 mean=training_data_dict['mean'],\n",
    "                                 std=training_data_dict['std'])\n",
    "\n",
    "# Create some parameter vectors for each feature set. Initialise to 0\n",
    "num_features = training_data_dict['features'].shape[1]\n",
    "num_train_samples = training_data_dict['features'].shape[0]\n",
    "\n",
    "# Train\n",
    "train_cv_split = 0.8\n",
    "idx = int(num_train_samples*train_cv_split)\n",
    "train_features = training_data_dict['features'][:idx]\n",
    "train_target = training_data_dict['target'][:idx]\n",
    "\n",
    "cv_features = training_data_dict['features'][idx:]\n",
    "cv_target = training_data_dict['target'][idx:]\n",
    "\n",
    "test_features = test_data_dict['features']\n",
    "test_target = test_data_dict['target']\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 3\n",
    "learning_rate = 0.03\n",
    "reg_lambda_range = np.arange(0.0, 0.01, 0.001)\n",
    "best_accuracy_reg = (0, 0)\n",
    "#cost_list = []\n",
    "\n",
    "tf.reset_default_graph()\n",
    "logreg = TFLogisticRegression(num_features, 1)\n",
    "debug = False\n",
    "with tf.Session() as sess:\n",
    "    if debug: sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    for reg_lambda in tqdm_notebook(reg_lambda_range, desc='Regularisation'):\n",
    "        # Reset the weights\n",
    "        logreg.initialise_graph(sess)\n",
    "\n",
    "        # Train the network\n",
    "        cost = logreg.train(sess,\n",
    "                            train_features,\n",
    "                            train_target,\n",
    "                            learning_rate,\n",
    "                            reg_lambda,\n",
    "                            train_features.shape[0],\n",
    "                            num_epochs)\n",
    "        tqdm.write(\"Labmda: %f\" % reg_lambda)\n",
    "        tqdm.write(\"Initial cost: %f\" % cost[0])\n",
    "        tqdm.write(\"Final cost: %f\" % cost[-1])\n",
    "        \n",
    "        # Crossvalidation\n",
    "        p = logreg.predict(sess, cv_features.T)\n",
    "        correct_test_predictions = np.logical_not(np.logical_xor(p > 0.5, cv_target > 0.5))\n",
    "        accuracy = correct_test_predictions.sum()/len(correct_test_predictions)\n",
    "        tqdm.write(\"CV accuracy: %f\\n\" % accuracy)\n",
    "        if accuracy > best_accuracy_reg[0]:\n",
    "            best_accuracy_reg = (accuracy, reg_lambda)\n",
    "            # TODO: Save model\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    #p = logreg.predict(sess, test_features.T)\n",
    "    #correct_test_predictions = np.logical_not(np.logical_xor(p > 0.5, test_target > 0.5))\n",
    "    #test_result_df = pd.DataFrame()\n",
    "    #test_result_df['predictions'] = pd.Series(p.flatten()>0.5)\n",
    "    #test_result_df['target'] = pd.Series(test_target.flatten() > 0.5)\n",
    "    #test_result_df['result'] = correct_test_predictions\n",
    "    #print(\"Success rate on test data:\", test_result_df.result.sum()/len(test_result_df))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | Accuracy: 0.376808 | Cost: 2.970554\n",
      "Iteration 250 | Accuracy: 0.366364 | Cost: 2.538901\n",
      "Iteration 500 | Accuracy: 0.344606 | Cost: 2.303133\n",
      "Iteration 750 | Accuracy: 0.325873 | Cost: 2.160696\n",
      "Lambda: 12.280000\n",
      "Hidden layer size:100\n",
      "Initial cost: 2.970554\n",
      "Final cost: 2.060103\n",
      "CV accuracy: 0.320404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ANN classification with TF\n",
    "# TODO: Implement logistic regression with TF\n",
    "def make_layer(name, input_tensor, num_input, num_output,\n",
    "               activation_function=None, loss_function=None):\n",
    "    n = num_input\n",
    "    weights = tf.Variable(dtype=tf.float32,\n",
    "                           initial_value=tf.random_normal([n, num_output]),\n",
    "                           trainable=True,\n",
    "                           name=name+\"_weights\")\n",
    "    bias = tf.Variable(dtype=tf.float32,\n",
    "                                   initial_value=tf.random_normal([num_output]),\n",
    "                                   trainable=True,\n",
    "                                   name=name+\"_bias\")\n",
    "    output = tf.add(tf.matmul(input_tensor, weights), bias)\n",
    "    if activation_function is not None:\n",
    "        act_output = activation_function(output)\n",
    "    else:\n",
    "        act_output = output\n",
    "        \n",
    "    if loss_function is not None:\n",
    "        loss = loss_function(weights)\n",
    "    else:\n",
    "        loss = 0\n",
    "        \n",
    "    d = {'name': name,\n",
    "         'weights': weights,\n",
    "         'bias': bias,\n",
    "         'output': output,\n",
    "         'act_output': act_output,\n",
    "         'loss': loss}\n",
    "    return d\n",
    "\n",
    "class TFLogisticRegressionNN():   \n",
    "    def __init__(self, num_features, num_classes, hidden_layers):\n",
    "        weights_maxval = 0.0001\n",
    "        # Classifier graph\n",
    "        self.input_node = tf.placeholder(dtype=tf.float32,\n",
    "                          shape=[None, num_features],\n",
    "                          name=\"feature_ph\")\n",
    "        \n",
    "        self.input_shape = tf.shape(self.input_node)\n",
    "        \n",
    "        # Make all of our hidden layers\n",
    "        self.hidden_layers = []\n",
    "        input_tensor = self.input_node\n",
    "        num_input = num_features\n",
    "        for name, hl_size, act_fun in hidden_layers:\n",
    "            d = make_layer(name,\n",
    "                           input_tensor,\n",
    "                           num_input,\n",
    "                           hl_size,\n",
    "                           act_fun,\n",
    "                           tf.nn.l2_loss)\n",
    "            input_tensor = d['act_output']\n",
    "            num_input = hl_size\n",
    "            self.hidden_layers.append(d)\n",
    "            \n",
    "        # Make the output layer\n",
    "        d = make_layer(\"output\",\n",
    "                       input_tensor,\n",
    "                       num_input,\n",
    "                       num_classes,\n",
    "                       tf.sigmoid,\n",
    "                       tf.nn.l2_loss)\n",
    "        self.output_layer = d\n",
    "        self.output = d['act_output']\n",
    "        self.output_logits = d['output']\n",
    "        \n",
    "        # Hidden Layer\n",
    "        #self.hl_weights = tf.Variable(dtype=tf.float32,\n",
    "        #                   initial_value=tf.random_normal([num_features,  hidden_layer_size]),\n",
    "        #                   trainable=True,\n",
    "        #                   name=\"hl_weights\")\n",
    "        #self.hl_bias = tf.Variable(dtype=tf.float32,\n",
    "        #                           initial_value=tf.random_normal([hidden_layer_size]),\n",
    "        #                           trainable=True,\n",
    "        #                           name=\"hl_bias\")\n",
    "        #self.hl_output = tf.add(tf.matmul(self.input_node,\n",
    "        #                                  self.hl_weights),\n",
    "        #                        self.hl_bias)\n",
    "        #self.hl_output = tf.sigmoid(self.hl_output, name=\"hl_output\")\n",
    "        \n",
    "        # Output Layer\n",
    "        #self.output_weights = tf.Variable(dtype=tf.float32,\n",
    "        #                                  initial_value=tf.random_normal([hidden_layer_size,  num_classes]),\n",
    "        #                                  trainable=True,\n",
    "        #                                  name=\"output_weights\")\n",
    "        #self.output_bias = tf.Variable(dtype=tf.float32,\n",
    "        #                               initial_value=tf.random_normal([num_classes]),\n",
    "        #                               trainable=True,\n",
    "        #                               name=\"output_bias\")\n",
    "        #self.output_logits = tf.add(tf.matmul(self.hl_output,\n",
    "        #                               self.output_weights),\n",
    "        #                            self.output_bias)\n",
    "        #self.output = tf.sigmoid(self.output_logits, name=\"output\")\n",
    "        \n",
    "        # Training\n",
    "        self.target = tf.placeholder(dtype=tf.float32,\n",
    "                                     shape=[None, num_classes],\n",
    "                                     name=\"target_ph\")\n",
    "        self.cost = tf.losses.sigmoid_cross_entropy(self.target,\n",
    "                                                    logits=self.output_logits)\n",
    "        self.cost = -tf.reduce_mean(self.target*tf.log(self.output))\n",
    "        self.reg_lambda = tf.placeholder(dtype=tf.float32,\n",
    "                                         name=\"lambda\")\n",
    "        reg_loss = 0\n",
    "        for d in self.hidden_layers:\n",
    "            reg_loss += d['loss']\n",
    "        reg_loss += self.output_layer['loss']\n",
    "        reg_loss *= tf.divide(self.reg_lambda,\n",
    "                             tf.to_float(self.input_shape[0]))\n",
    "            \n",
    "        #self.reg_loss = tf.nn.l2_loss(self.hl_weights) + tf.nn.l2_loss(self.output_weights)\n",
    "        self.cost = self.cost + reg_loss\n",
    "        \n",
    "        self.learning_rate = tf.placeholder(dtype=tf.float32,\n",
    "                                             name=\"learning_rate\")\n",
    "        #self.update = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.update = self.update.minimize(self.cost)\n",
    "        \n",
    "    def initialise_graph(self, sess):\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "    def predict(self, sess, x):\n",
    "        return sess.run(self.output,\n",
    "                        feed_dict={self.input_node:x})\n",
    "    \n",
    "    \n",
    "    def _train_step(self, sess, _x, _y, learning_rate, reg_lambda):\n",
    "        feed_dict = {self.input_node: _x,\n",
    "                     self.target: _y,\n",
    "                     self.learning_rate: learning_rate,\n",
    "                     self.reg_lambda: reg_lambda}\n",
    "        cost, _, _ = sess.run([self.cost, self.update, self.output],\n",
    "                           feed_dict=feed_dict)\n",
    "        return cost\n",
    "    \n",
    "    def train(self, sess, x, y, learning_rate, reg_lambda, batch_size, num_epochs):\n",
    "        num_batches = int(np.floor(x.shape[0]/batch_size))\n",
    "        batch_remainder = x.shape[0] % batch_size\n",
    "        cost_list = []\n",
    "        for i in tqdm_notebook(range(num_epochs), desc='Training'):\n",
    "            for j in range(num_batches):\n",
    "                k = j * batch_size\n",
    "                _x = np.matrix(x[k:k+batch_size-1])\n",
    "                _y = np.matrix(y[k:k+batch_size-1])\n",
    "                cost = self._train_step(sess, _x, _y, learning_rate, reg_lambda)\n",
    "                cost_list.append(cost)\n",
    "            if (batch_remainder > 0):\n",
    "                # Do remainder\n",
    "                k = -batch_remainder\n",
    "                _x = np.matrix(x[k:])\n",
    "                _y = np.matrix(y[k:])\n",
    "                cost = self._train_step(sess, _x, _y, learning_rate, reg_lambda)\n",
    "                cost_list.append(cost)\n",
    "            \n",
    "            #p = self.predict(sess, x)\n",
    "            #pr = np.logical_not(np.logical_xor(p > 0.5, y > 0.5))\n",
    "            #accuracy = pr.sum()/len(pr)\n",
    "            #tqdm.write(str(p[5:10]))\n",
    "            #tqdm.write(str(pr[5:10]))\n",
    "            #tqdm.write(str(y[5:10]))\n",
    "            if i % 250 == 0:\n",
    "                p = self.predict(sess, x)\n",
    "                pr = np.logical_not(np.logical_xor(p > 0.5, y > 0.5))\n",
    "                accuracy = pr.sum()/len(pr)\n",
    "                tqdm.write(\"Iteration %i | Accuracy: %f | Cost: %f\" % (i, accuracy, cost_list[-1]))\n",
    "\n",
    "        return cost_list\n",
    "    \n",
    "def cross_validate(sess, x, y):\n",
    "    p = logreg.predict(sess, x)\n",
    "    correct_test_predictions = np.logical_not(np.logical_xor(p > 0.5, y > 0.5))\n",
    "    accuracy = correct_test_predictions.sum()/len(correct_test_predictions)\n",
    "    return accuracy\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "#LNN = TFLogisticRegressionNN(5, 1)\n",
    "#init = tf.global_variables_initializer()\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR + DATA_FILE, names=NAMES)\n",
    "test_df = pd.read_csv(DATA_DIR + TEST_FILE, names=NAMES, skiprows=1)\n",
    "\n",
    "training_data_dict = preprocess_data(train_df)\n",
    "test_data_dict = preprocess_data(test_df,\n",
    "                                 feature_list=training_data_dict['feature_list'],\n",
    "                                 mean=training_data_dict['mean'],\n",
    "                                 std=training_data_dict['std'])\n",
    "\n",
    "# Create some parameter vectors for each feature set. Initialise to 0\n",
    "num_features = training_data_dict['features'].shape[1]\n",
    "num_train_samples = training_data_dict['features'].shape[0]\n",
    "\n",
    "# Train\n",
    "train_cv_split = 0.8\n",
    "idx = int(num_train_samples*train_cv_split)\n",
    "train_features = training_data_dict['features'][:idx]\n",
    "train_target = training_data_dict['target'][:idx]\n",
    "\n",
    "cv_features = training_data_dict['features'][idx:]\n",
    "cv_target = training_data_dict['target'][idx:]\n",
    "\n",
    "test_features = test_data_dict['features']\n",
    "test_target = test_data_dict['target']\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = train_features.shape[0]\n",
    "learning_rate = 0.0001\n",
    "#reg_lambda_range = np.arange(0.0, 1, 0.1)\n",
    "#reg_lambda_range = [0.00, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.28]\n",
    "base = 10.28\n",
    "#reg_lambda_range = [base+x for x in range(5)]\n",
    "reg_lambda_range = [12.28]\n",
    "#hidden_layer_range = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "hidden_layer_range = [100]\n",
    "best_cv_accuracy = 0\n",
    "best_accuracy_reg = (0, 0)\n",
    "best_accuracy_hl = (0, 0)\n",
    "#cost_list = []\n",
    "\n",
    "\n",
    "debug = False\n",
    "for reg_lambda in tqdm_notebook(reg_lambda_range, desc='Regul'):\n",
    "    for hl_size in hidden_layer_range:\n",
    "        tf.reset_default_graph()\n",
    "        hidden_layers = [(\"hl1\", hl_size, tf.sigmoid)]#,\n",
    "                         #(\"hl2\", 30, tf.sigmoid)]\n",
    "        logreg = TFLogisticRegressionNN(num_features, 1, hidden_layers)\n",
    "        with tf.Session() as sess:\n",
    "            if debug: sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "            # Reset the weights\n",
    "            logreg.initialise_graph(sess)\n",
    "\n",
    "            # Train the network\n",
    "            cost = logreg.train(sess,\n",
    "                                train_features,\n",
    "                                train_target,\n",
    "                                learning_rate,\n",
    "                                reg_lambda,\n",
    "                                #train_features.shape[0],\n",
    "                                batch_size,\n",
    "                                num_epochs)\n",
    "            tqdm.write(\"Lambda: %f\" % reg_lambda)\n",
    "            tqdm.write(\"Hidden layer size:%i\" % hl_size)\n",
    "            tqdm.write(\"Initial cost: %f\" % cost[0])\n",
    "            tqdm.write(\"Final cost: %f\" % cost[-1])\n",
    "            accuracy = cross_validate(sess, cv_features, cv_target)\n",
    "            tqdm.write(\"CV accuracy: %f\\n\" % accuracy)\n",
    "            if accuracy > best_cv_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_accuracy_reg = reg_lambda\n",
    "                best_accuracy_hl = hl_size\n",
    "    \n",
    "    #p = logreg.predict(sess, test_features)\n",
    "    #correct_test_predictions = np.logical_not(np.logical_xor(p > 0.5, test_target > 0.5))\n",
    "    #test_result_df = pd.DataFrame()\n",
    "    #test_result_df['predictions'] = pd.Series(p.flatten()>0.5)\n",
    "    #test_result_df['target'] = pd.Series(test_target.flatten() > 0.5)\n",
    "    #test_result_df['result'] = correct_test_predictions\n",
    "    #print(\"Success rate on test data:\", test_result_df.result.sum()/len(test_result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAH1tJREFUeJzt3XmUXGWd//H3NzthSQKGfREMSxREumPYJDDCgDiCPxCB\nBlyOBxVXzBxHRXTQUeeACyigjDIIytIjsjiAC7KoKEGQbgYQAhpkESQhQmiWQNbn98fTkU5zE1Kd\nqnurut6vc+4p6tater79dJP+9HOfe59IKSFJkjTYiKoLkCRJzcmQIEmSChkSJElSIUOCJEkqZEiQ\nJEmFDAmSJKmQIUGSJBUyJEiSpEKGBEmSVMiQIEmSCtUcEiJi84i4MCL+HhELI+LOiOhoRHGSJKk6\no2o5OCImAjcDNwAHAX8HtgcW1L80SZJUpahlgaeIOBXYM6W0b+NKkiRJzaDW0w2HALdHxKURMS8i\neiPi+EYUJkmSqlXrSMILQAK+AVwGTAe+BXwwpXRhwfEbkU9LPAS8WId6JUlqF+OAVwPXppSerKKA\nWkPCIuC2lNI+A/Z9C5iWUtq74PhjgIvrUagkSW3q2JTSJVU0XNPEReBxYPagfbOBw1dx/EMAF110\nEVOnTq2xKQ3VzJkzOeOMM6ouo63Y5+Wzz8tnn5dr9uzZHHfccdD/u7QKtYaEm4EdB+3bEXh4Fce/\nCDB16lQ6OrxKsiwTJkywv0tmn5fPPi+ffV6Zyk7X1zpx8Qxgj4g4KSJe03864Xjg7PqXJkmSqlRT\nSEgp3Q4cBnQBdwMnAyemlP6nAbVJkqQK1Xq6gZTSz4CfNaAWSZLURFy7YRjq6uqquoS2Y5+Xzz4v\nn33efmq6BLLmD89rOvT09PQ42UWSpBr09vbS2dkJ0JlS6q2iBkcSJElSIUOCJEkqZEiQJEmFDAmS\nJKmQIUGSJBUyJEiSpEKGBEmSVMiQIEmSChkSJElSIUOCJEkqZEiQJEmFDAmSJKmQIUGSJBUyJEiS\npEKGBEmSVMiQIEmSChkSJElSIUOCJEkqZEiQJEmFDAmSJKmQIUGSJBUyJEiSpEKGBEmSVMiQIEmS\nChkSJElSIUOCJEkqZEiQJEmFDAmSJKmQIUGSJBUyJEiSpEKGBEmSVMiQIEmSChkSJElSIUOCJEkq\nZEiQJEmFDAmSJKmQIUGSJBUyJEiSpEKGBEmSVKimkBARp0TE8kHbvY0qTpIkVWfUEN7zR2B/IPqf\nL61fOZIkqVkMJSQsTSnNr+kNxghJklrOUOYkbB8Rj0XEAxFxUURs9UpvmF9TpJAkSc2g1pDwe+C9\nwEHACcC2wE0Rse7q3rRkyZBqkyRJFYqU0tDfHDEBeBiYmVI6v+D1DqCns3MGm28+YaXXurq66Orq\nGnLbkiQNF93d3XR3d6+0r6+vj5tuugmgM6XUW0VdaxUSACLiNuC6lNLJBa91AD2XXNJDV1fHWrUj\nSVI76e3tpbOzEyoMCWt1n4SIWA+YAjy+uuM83SBJUuup9T4JX4uIGRGxTUTsBVwJLAG6V/c+r26Q\nJKn11HoJ5JbAJcBGwHzgd8AeKaUnV/cmRxIkSWo9NYWElNKQZhoaEiRJaj2lrN3g6QZJklqPIUGS\nJBUyJEiSpEKlhATnJEiS1HoMCZIkqZCnGyRJUiFHEiRJUiFDgiRJKuTpBkmSVMiRBEmSVMiRBEmS\nVMiQIEmSCnm6QZIkFXIkQZIkFXIkQZIkFXIkQZIkFXIkQZIkFTIkSJKkQp5ukCRJhRxJkCRJhRxJ\nkCRJhQwJkiSpkKcbJElSIUcSJElSIUcSJElSIUcSJElSIUcSJElSIUOCJEkq5OkGSZJUyJEESZJU\nyJEESZJUqJSQsHhxGa1IkqR6KiUkLF/uaIIkSa2mlJAA8OKLZbUkSZLqwZAgSZIKGRIkSVIhQ4Ik\nSSpkSJAkSYUMCZIkqZAhQZIkFVqrkBARn4mI5RFx+isda0iQJKm1DDkkRMQbgQ8Ad67J8YYESZJa\ny5BCQkSsB1wEHA88vSbvMSRIktRahjqS8G3g6pTSjWv6hhdeGGJLkiSpEqNqfUNEHA28AZhWy/sc\nSZAkqbXUFBIiYkvgm8ABKaUla/q+MWMMCZIktZpaRxI6gclAb0RE/76RwIyI+CgwNqWUBr9p2bKZ\nnHPOBK677qV9XV1ddHV1Da1qSZKGke7ubrq7u1fa19fXV1E1L4mC3+mrPjhiXWCbQbsvAGYDp6aU\nZg86vgPo2WijHk48sYPPf34tq5UkqU309vbS2dkJ0JlS6q2ihppGElJKzwP3DtwXEc8DTw4OCAN5\nukGSpNZTjzsuvuJQxNixhgRJklpNzVc3DJZSevMrHeNIgiRJraeUtRvGjvU+CZIktZpSQsI668Dz\nz5fRkiRJqpdSQsL48fDcc2W0JEmS6qW0kQRDgiRJrcWRBEmSVMiQIEmSCjlxUZIkFXJOgiRJKlTq\n6YYalomQJEkVKy0kLFsGixaV0ZokSaqH0k43gKccJElqJaWNJIAhQZKkVmJIkCRJhUoJCePG5UdD\ngiRJraOUkLDuuvnx2WfLaE2SJNVDKSFh/fXz44IFZbQmSZLqobSRhBEjDAmSJLWSUkLCiBEwaRI8\n9VQZrUmSpHooJSRADgmOJEiS1DpKCwkbbuhIgiRJraTUkQRDgiRJraPUkQRPN0iS1Do83SBJkgo5\ncVGSJBUqdSThySfLak2SJK2t0kLCJpvAwoWu3yBJUqsoLSRsuml+fPzxslqUJElro7SQsNlm+XHu\n3LJalCRJa8ORBEmSVKi0kDBxIowd60iCJEmtorSQEJFPOTiSIElSaygtJEA+5WBIkCSpNZQaEjbf\nHB57rMwWJUnSUJUaEl79anj44TJblCRJQ1VJSFi+vMxWJUnSUJQaErbdFhYvdl6CJEmtoPSRBIAH\nHyyzVUmSNBSVhISHHiqzVUmSNBSlhoT11oPJkx1JkCSpFZQaEiCPJhgSJElqfqWHhB12gPvvL7tV\nSZJUq5pCQkScEBF3RkRf/zYrIt5Sy2dMnQqzZ0NKtRUqSZLKVetIwl+BTwMdQCdwI/C/ETF1TT9g\n6lRYsACeeKLGliVJUqlqCgkppZ+mlH6RUnogpTQnpfQ54DlgjzX9jJ12yo+zZ9fSsiRJKtuQ5yRE\nxIiIOBoYD9yypu+bMgVGjoT77htqy5IkqQyjan1DROxMDgXjgGeBw1JKa/wrf8yYHBQcSZAkqbnV\nHBKA+4BdgQnAEcAPI2LG6oLCzJkzmTBhwj+e9/XBjTd2AV1DaF6SpOGlu7ub7u7ulfb19fVVVM1L\nIq3lZQYRcR0wJ6X0oYLXOoCenp4eOjo6/rH/lFPgnHNg3jyIWKvmJUkalnp7e+ns7AToTCn1VlFD\nPe6TMAIYW8sbdtsN5s+Hv/2tDq1LkqSGqOl0Q0T8J/Bz4BFgfeBYYF/gwFo+Z7fd8mNvL2yxRS3v\nlCRJZal1JGFj4AfkeQnXk++VcGBK6cZaPmTrrWHDDeGOO2psXZIklaamkYSU0vH1aDQijyYYEiRJ\nal6lr92wgiFBkqTmVmlIePhhePLJqiqQJEmrU1lI2H33/HjrrVVVIEmSVqeykLDddrDxxnDLGt/Q\nWZIklamykBABe+0Fs2ZVVYEkSVqdykICwJ575tMNS5dWWYUkSSpSaUjYay94/nn44x+rrEKSJBWp\nNCR0dsLo0Z5ykCSpGVUaEtZZBzo64He/q7IKSZJUpNKQALDffvCrX8FaLkYpSZLqrPKQsP/+MHcu\nzJ5ddSWSJGmgykPC3nvDmDFwww1VVyJJkgaqPCSMH58vhbyxpnUkJUlSo1UeEiCfcvj1r2HZsqor\nkSRJKzRFSHjzm+Hpp6G3t+pKJEnSCk0REqZPh/XXh2uvrboSSZK0QlOEhNGj4cAD4ac/rboSSZK0\nQlOEBIC3vS2v4zB/ftWVSJIkaKKQcPDB+YZKP/951ZVIkiRoopCwySZ5bsI111RdiSRJgiYKCQD/\n8i958uKSJVVXIkmSmi4kPPMM/Pa3VVciSZKaKiR0dMBWW8EVV1RdiSRJaqqQEAFHHAGXX+7dFyVJ\nqlpThQTIIWHuXJg1q+pKJElqb00XEvbYAzbfHC67rOpKJElqb00XEkaMgHe8I59yWL686mokSWpf\nTRcSAN75TnjssXwHRkmSVI2mDAl77QWbbgqXXlp1JZIkta+mDAkjR8JRR0F3NyxdWnU1kiS1p6YM\nCQDvehfMmwc33FB1JZIktaemDQkdHbDTTnDhhVVXIklSe2rakBCRRxOuvBKee67qaiRJaj9NGxIA\njj0WFi70Ns2SJFWhqUPCNtvAvvt6ykGSpCo0dUiAfMrhhhvg0UerrkSSpPbS9CHhne+EddaB88+v\nuhJJktpL04eEDTaAo4+G885zZUhJksrU9CEB4AMfgIcfhuuuq7oSSZLaR0uEhOnTYZdd4Nxzq65E\nkqT20RIhIQLe/3646iqYO7fqaiRJag81hYSIOCkibouIZyJiXkRcGRE7NKq4gY47DkaNggsuKKM1\nSZJU60jCPsBZwO7AAcBo4JcRsU69Cxts0qR8pcO558Ly5Y1uTZIk1RQSUkpvTSldmFKanVK6G3gv\nsDXQ2YjiBvvQh+Avf4Gf/7yM1iRJam9rOydhIpCAp+pQyyvaYw+YNg3OPLOM1iRJam9DDgkREcA3\ngd+llO6tX0mraxM+/nH45S/hvvvKaFGSpPYVKaWhvTHiHOAgYO+U0uOrOKYD6JkxYwYTJkxY6bWu\nri66urpqbnfRorymwxFHwNlnD6FwSZKaTHd3N93d3Svt6+vr46abbgLoTCn1VlHXkEJCRJwNHALs\nk1J6ZDXHdQA9PT09dHR0DL3KQb7wBfj61+Gxx2BQ9pAkaVjo7e2ls7MTKgwJNZ9u6A8Ibwf+aXUB\noZE++EFYvNj1HCRJaqRa75PwHeBY4Bjg+YjYpH8b15DqVmGzzeDII+Gss1zPQZKkRql1JOEEYAPg\n18DfBmxH1resVzZzZr4c8oorym5ZkqT2UOt9EkaklEYWbD9sVIGr0tkJBxwAp54KQ5x7KUmSVqMl\n1m5YlU9/Gnp74frrq65EkqThp6VDwv775xGF006ruhJJkoaflg4JEXk04YYb4Pbbq65GkqThpaVD\nAsDhh8OUKY4mSJJUby0fEkaOhE99Ci6/HO65p+pqJEkaPlo+JAC85z2w9dbwxS9WXYkkScPHsAgJ\nY8bA5z4HP/4x3HVX1dVIkjQ8DIuQAHk0YdttHU2QJKlehk1IGD0a/v3f8x0Y77ij6mokSWp9wyYk\nABx3XL7S4ZRTqq5EkqTWN6xCwqhROSBcfTXMmlV1NZIktbZhFRIAjjkGdt0V/u3fXNNBkqS1MexC\nwogR8LWv5ZGEK6+suhpJklrXsAsJAP/8z3DQQfCZz8CSJVVXI0lSaxqWIQHgq1+FOXPge9+ruhJJ\nklrTsA0Jr389vPe98IUvQF9f1dVIktR6hm1IAPjSl+CFF3JQkCRJtRnWIWGLLeDzn4ezzoK77666\nGkmSWsuwDgkAM2fmGyx99KNeEilJUi2GfUgYMwbOPBNuugm6u6uuRpKk1jHsQwLAgQfC4YfDJz8J\nzzxTdTWSJLWGtggJAGecka9y+Nznqq5EkqTW0DYhYeut4StfgbPPhptvrroaSZKaX9uEBICPfQym\nT4fjj4cXX6y6GkmSmltbhYSRI+G88+CBB+DLX666GkmSmltbhQSA170uz0s47TS4886qq5EkqXm1\nXUiAvPDT1Knwrnd52kGSpFVpy5AwZgxcdBHcfz+cfHLV1UiS1JzaMiRAXgDq1FPh9NPh+uurrkaS\npObTtiEB4MQT4YAD4D3vgaeeqroaSZKaS1uHhBEj4IIL8kqR73+/aztIkjRQW4cEyCtFnnceXHFF\nXuNBkiRlbR8SAA47DP71X/PaDrfcUnU1kiQ1B0NCv1NPzXdjPPJImD+/6mokSaqeIaHf6NFw6aWw\naBEceywsW1Z1RZIkVcuQMMAWW0B3N9xwQ77hkiRJ7cyQMMj+++d7J3z96/D971ddjSRJ1RlVdQHN\n6OMfh3vvhRNOgClTYMaMqiuSJKl8jiQUiICzz4Y3vQkOPzyvGilJUrsxJKzC6NFw2WUwaRIcfDA8\n8UTVFUmSVK6aQ0JE7BMRV0XEYxGxPCIObURhzWDDDeHaa+HZZ3NQeOaZqiuSJKk8QxlJWBf4P+DD\nwLC/kfF228EvfgFz5uSbLi1aVHVFkiSVo+aQkFL6RUrp31NK/wtEA2pqOrvuCldfDTffnO+hsHRp\n1RVJktR4zklYQzNmwI9+BD/5Cbz73QYFSdLwZ0iowdvfnoPCpZcaFCRJw58hoUbveIdBQZLUHkq5\nmdLMmTOZMGHCSvu6urro6uoqo/m6WxEUjjoKFi+Giy+GsWOrrkqS1Kq6u7vp7u5eaV9fX19F1bwk\nUhr6BQoRsRz4fymlq1bxegfQ09PTQ0dHx5DbaVZXXZWDwp575rkKG2xQdUWSpOGit7eXzs5OgM6U\nUm8VNQzlPgnrRsSuEfGG/l3b9T/fqs61Nb1DD833Uejthf32g3nzqq5IkqT6GcqchGnAHUAP+T4J\n3wB6gS/Wsa6WMWMG3HQTPP447L033H9/1RVJklQfQ7lPwm9SSiNSSiMHbe9rRIGt4PWvh1mzYMwY\n2GMPuO66qiuSJGnteXVDnWy7LdxyS56fcPDBcNZZsBbTPSRJqpwhoY4mTMh3ZvzEJ/Jy0+9/P7zw\nQtVVSZI0NIaEOhs5Er7+dTj//Hxp5O67O09BktSaDAkN8t73wm23wZIlMG0aDLr8VZKkpmdIaKBd\ndoE//CHfzvmYY3JwePrpqquSJGnNGBIabL314MIL8+mHK6/MwcGrHyRJrcCQUIKIPIpw992w445w\n4IHw4Q/Ds89WXZkkSatmSCjR1lvDL38J3/42/OAHsNNOeaEoL5WUJDUjQ0LJRozIowj33gvTp+e1\nH97yFvjzn6uuTJKklRkSKrLNNnmOwtVXw5/+BDvvDJ/6FCxYUHVlkiRlhoSKve1tcM898NnPwne+\nA695DZx+OixaVHVlkqR2Z0hoAuPHwymn5FMORx6ZRxR22ilfEbFkSdXVSZLalSGhiWy2GfzXf+Wr\nIDo64H3vgx12gO99z5EFSVL5DAlNaOpUuPxyuOuuPLnxhBNgyhT41rfgmWeqrk6S1C4MCU1sl13g\nRz/Kcxb23Rc++UnYcsu8gNQDD1RdnSRpuDMktICpU+Gii+Chh+BjH8v/vf32cOihcM01sHRp1RVK\nkoYjQ0IL2WIL+MpX4K9/zfMUHnkEDjkk36TppJPypZSSJNWLIaEFrbMOHH883HEH9PTA4YfnCY87\n7gh77w1nngmPPVZ1lZKkVmdIaGER+SqIs8+Gxx+HSy6BSZNemruwzz4GBknS0BkSholx46CrK89R\neOIJuOACmDDhpcCw225w8slw883OYZAkrRlDwjA0cSK85z05MMybl0cYdt45z2N405tg443h6KPh\nu9+F++5zgSlJUrFRVRegxpo0KY8wdHXBsmVw++3ws5/BtdfCRz6S922yCcyYAfvtl0PEa18Lo/zJ\nkKS256+CNjJyJOy+e96++EV49lmYNQt+8xv49a/hxBPzqYjx4/Nch+nT4Y1vzNt22+U5EJKk9mFI\naGPrrw8HHZQ3gOefz1dL/OEPcNtteZXK00/Pr22wQT5lMXibPLm6+iVJjWVI0D+su24+7TBjxkv7\n5s/PpyjuuiuvKXHrrXlS5OLF+fXJk/P6ElOmvHybOLGSL0OSVCeGBK3W5Mlw8MF5W2HpUpgzB/74\nx7zNmZMnQF5zDTz55EvHbbQRbLVVvrpi8OOWW+abQ40fX/7XJElaM4YE1WzUqLyU9U47wRFHrPza\nggV5XYk5c/Ljo4/mO0TOmpX/e2CIgDx6MXlyvuJixTbw+UYb5RGJgdu4cc6PkKQyGBJUV5MmwbRp\neSuycGG+udOjj+bH+fPzfR1WPN5zT3584ok8R6LImDEvDw7rr58Dx4ptvfVWfl60jR378s2rOiTp\nJf6TqFKNH58Xp9p++1c+duFCeOopePpp6OvLj0XbggX5So25c3OwGLzVcvOoESOKw0NRmBi4jR79\n8n2r2wYfP3JkbnvFNvj5qrY1OW5NP2vEiDxCszZbPT5jxSapeoYENa3x4/O25ZZr9zmLF8Nzz60c\nHBYuhEWLXtpefHHl56+0LVuWw8fSpfm9zz0HS5a8tG8oW0r5c5WVEVhWtDOwzdU9DvW1Mt4/XNpY\n0/cPVLS/lmPr8RmNOHbBguLjymRI0LA3ZgxsuGHeWkFKsHz5y7dly4r3r8nra/relNZ+q8fnlFHL\nir4e2O+rexzqa2W8f7i0sabvH2h1d4yt5T0Dfy7W5nPquX9Vp1zLZEiQmkxEPkUwcmTVlUiqUm8v\ndHZWW4NrN0iSpEKGBEmSVMiQIEmSChkSJElSIUOCJEkqZEiQJEmFDAmSJKmQIUGSJBUyJAxD3d3d\nVZfQduzz8tnn5bPP28+QQkJEfCQiHoyIFyLi9xHxxnoXpqHzf+Ty2efls8/LZ5+3n5pDQkQcBXwD\nOAXYDbgTuDYiXlXn2iRJUoWGMpIwE/huSumHKaX7gBOAhcD76lqZJEmqVE0hISJGA53ADSv2pZQS\ncD2wZ31LkyRJVap1FchXASOBeYP2zwN2LDh+HMDs2bNrr0xD1tfXR29vb9VltBX7vHz2efns83IN\n+N05rqoaIq1uMe7BB0dsBjwG7JlSunXA/tOAGSmlPQcdfwxwcZ1qlSSpHR2bUrqkioZrHUn4O7AM\n2GTQ/k2AuQXHXwscCzwEvFhrcZIktbFxwKvJv0srUdNIAkBE/B64NaV0Yv/zAB4Bzkwpfa3+JUqS\npCrUOpIAcDpwQUT0ALeRr3YYD1xQx7okSVLFag4JKaVL+++J8B/k0wz/BxyUUppf7+IkSVJ1aj7d\nIEmS2oNrN0iSpEKGBEmSVKihIcGFoIYmIk6KiNsi4pmImBcRV0bEDgXH/UdE/C0iFkbEdRExZdDr\nYyPi2xHx94h4NiIui4iNBx0zKSIujoi+iFgQEf8dEes2+mtsZhHxmYhYHhGnD9pvf9dZRGweERf2\n99nCiLgzIjoGHWO/10lEjIiIL0XEX/r7c05EfK7gOPt8iCJin4i4KiIe6/935NCCY0rp34jYKiJ+\nGhHPR8TciPhqRNT2ez+l1JANOIp8b4R3AzsB3wWeAl7VqDaHywb8DHgXMBXYBbiGfK+JdQYc8+n+\n/nwbsDPwE+ABYMyAY87pf9++5MW4ZgG/HdTWz4FeYBqwF/An4KKq+6DCvn8j8BfgDuB0+7uhfT0R\neBD4b/Lt3rcBDgC2td8b1uefBZ4A3gJsDRwOPAN81D6vWx+/hTyx/+3k+wodOuj1UvqXPAhwN/ke\nC7sAB/V/779c09fTwI76PfCtAc8DeBT4VNXfxFbbyLfDXg68acC+vwEzBzzfAHgBOHLA80XAYQOO\n2bH/c6b3P5/a/3y3AcccBCwFNq36666gn9cD7gfeDPyKlUOC/V3//j4V+M0rHGO/17fPrwbOHbTv\nMuCH9nlD+ns5Lw8JpfQvcDCwhAF/mAMfBBYAo9b0a2jI6YZwIah6mwgkcvokIrYFNmXl/n0GuJWX\n+nca+RLXgcfcT77x1Ypj9gAWpJTuGNDW9f1t7d6IL6TJfRu4OqV048Cd9nfDHALcHhGX9p9W642I\n41e8aL83xCxg/4jYHiAidgX2Jo9e2ucNVnL/7gHcnVL6+4BjrgUmAK9b05qHcjOlNVHrQlBahYgI\n4JvA71JK9/bv3pT8w1DUv5v2//cmwOL+H8BVHbMpefjpH1JKyyLiqQHHtIWIOBp4A/l/0MHs78bY\nDvgQ8A3gK8B04MyIWJRSuhD7vRFOJf+lel9ELCMPSZ+cUvqf/tft88Yqs383XUU7K167c00KblRI\nUP18B3gtOe2rASJiS3IQOyCltKTqetrICOC2lNLn+5/fGRE7AycAF1ZX1rB2FHAMcDRwLzkYfysi\n/tYfzKSVNOrqhloXglKBiDgbeCuwX0rp8QEvzSXP8Vhd/84FxkTEBq9wzOAZsyOBDWmv71MnMBno\njYglEbGEPGHoxIhYTE7f9nf9PQ4MXkd+NnlCHfhz3ghfBU5NKf04pXRPSuli4AzgpP7X7fPGKrN/\n566iHajhe9CQkND/11gPsP+Kff3D5vuTz4npFfQHhLcD/5RSemTgaymlB8nf5IH9uwH5XNSK/u0h\nT2IZeMyO5H+Ab+nfdQswMSJ2G/Dx+5N/iG+lfVxPnv37BmDX/u124CJg15TSX7C/G+FmXn76cUfg\nYfDnvEHGk/+AG2g5/b8L7PPGKrl/bwF2ibyMwgoHAn3kUaQ1LrpRszqPBBay8iWQTwKTq55x2uwb\n+RTDAmAfcvJbsY0bcMyn+vvzEPIvuJ8Af2bly2i+Q77EbD/yX8s38/LLaH5G/oX4RvIpjfuBC6vu\ng6o3Xn51g/1d/z6eRp7FfRLwGvIw+LPA0fZ7w/r8fPIEuLeSLzk9jHxu+z/t87r18brkPzTeQA5g\nn+h/vlWZ/UsOfneSL5V8Pfnqh3nAl2r6ehrcWR8mX+v5AjnVTKv6G9gKW/8P1rKC7d2DjvsC+XKa\nheRZq1MGvT4WOIt8+udZ4MfAxoOOmUj+i7mPHEzOBcZX3QdVb8CNDAgJ9nfD+vmtwF39fXoP8L6C\nY+z3+vX3uuSVfB8Enu//5fRFBl0SZ5+vVR/vu4p/w79fdv8CW5Hvs/McOSCcBoyo5etxgSdJklTI\ntRskSVIhQ4IkSSpkSJAkSYUMCZIkqZAhQZIkFTIkSJKkQoYESZJUyJAgSZIKGRIkSVIhQ4IkSSpk\nSJAkSYX+P1EZ4AFNq+G0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b9667c358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.matrix(train_features[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost[1500:1700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = None\n",
    "if f != None: print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
